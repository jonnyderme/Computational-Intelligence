%% Aristotle University of Thessaloniki (AUTh)
% Department of Electrical and Computer Engineering
%% Ioannis Deirmentzoglou AEM: 10015 Email: deirmentz@ece.auth.gr 

%% Assignment 3 Part 2 
% Import dataset from the specified path
dataset = importdata('C:\Thmmy_Auth\Computational_Intelligence\Datasets\superconduct.csv');
datasetTarget = dataset(:, end);  % Target values (last column of the dataset)

% Dataset split into 60% training, 20% validation, and 20% testing sets
[trainData, validationData, testData] = split_scale(dataset, 1);  % Assuming split_scale is a custom function
trainTargetData = trainData(:, end);  % Target values for the training set
validationTargetData = validationData(:, end);  % Target values for the validation set
testTargetData = testData(:, end);  % Target values for the test set

%% Grid Search Algorithm 
% Initialization of variables for grid search to find the optimal number of features and cluster radius.
% The objective is to minimize Mean Squared Error (MSE).

% Array of different numbers of features to evaluate
numFeatures = [4 8 10 12 15] ;

% Array of different cluster radii to evaluate for subtractive clustering
clusterRadius = [0.2 0.4 0.6 0.8 1];

metricsValues = zeros(length(numFeatures),length(clusterRadius),4) ; 

% Number of folds for k-fold cross-validation
folds = 5;

% Method for clustering to generate fuzzy if-then rules
clusterMethod = 'SubtractiveClustering';

% Features selection using the ReliefF algorithm with k-nearest neighbors
numNearestNeighbors = 10;
[featureIndices, importanceWeights] = relieff(dataset(:, 1:end-1), datasetTarget, numNearestNeighbors, 'method', 'regression');

% Initialize matrices to hold MSE values and number of rules for each grid search combination
MSEValues = zeros(length(numFeatures), length(clusterRadius));
numRulesMat = zeros(length(numFeatures), length(clusterRadius));

% Iterate through every combination of number of features and cluster radius
iCount = 1 ;
for i = numFeatures
    % Creation of train and test sets based on the indices of selected fetures
    trainSet = [trainData(:, featureIndices(1:i)) trainTargetData];
    testSet = [testData(:, featureIndices(1:i)) testTargetData];
    jCount = 1 ;
    
    for j = clusterRadius
        % Create a random partition of the dataset for k-fold cross-validation
        cvObj = cvpartition(trainSet(:, end), 'KFold', folds);  % Partition data for k-fold CV
        % Array to store metrics values for each fold
        cvMSE = zeros(1, folds) ;
        cvRMSE = zeros(1,folds) ; 
        cvNDEI = zeros(1,folds) ;
        cvR2 = zeros(1,folds) ; 
        cvNMSE = zeros(1,folds) ;

        % K fold cross-validation
        for k = 1:folds
            % Training data for the k-th fold
            TrainDataFold = trainSet(training(cvObj, k), :);
            cvTrainTargetData = TrainDataFold(:, end);  % Target values for training

            % Validation data for the k-th fold
            valid = trainSet(test(cvObj, k), :);

            % Generate initial fuzzy inference system (FIS) using subtractive clustering
            fisOptions = genfisOptions(clusterMethod, 'ClusterInfluenceRange', j);  % Set cluster radius
            initialFIS = genfis(TrainDataFold(:, 1:end-1), cvTrainTargetData, fisOptions);

            % Set options for ANFIS (Adaptive Neuro-Fuzzy Inference System) training
            ANFISoptions = anfisOptions;
            ANFISoptions.InitialFIS = initialFIS;  % Use the generated FIS as the initial FIS
            ANFISoptions.ValidationData = valid;  % Validation data for tuning the model
            ANFISoptions.EpochNumber = 50;  % Number of epochs for training

            % Train the ANFIS model and update the FIS
            [~, ~, ~, fis, ~] = anfis(TrainDataFold, ANFISoptions);
            % Evaluate the trained FIS on the test data and calculate the predicted output
            yPred = evalfis(fis, testSet(:, 1:end-1));

            % Calculate the Mean Squared Error for the predictions
            cvMSE(:, k) = mse(yPred, testTargetData);
            % Calculate the rest metrics 
            cvNMSE(:,k) = NMSE(yPred, testTargetData) ; 
            cvRMSE(:,k) = RMSE(yPred, testTargetData) ;
            cvR2(:,k) = R2(yPred, testTargetData) ;
            cvNDEI(:,k) = NDEI(yPred, testTargetData) ;
        end

        % Calculate the mean MSE over all folds for the current feature number and cluster radius
        MSEValues(find(numFeatures == i), find(clusterRadius == j)) = mean(cvMSE(1, :));
        % Store the number of rules generated by the FIS for this feature and cluster radius combination
        numRulesMat(find(numFeatures == i), find(clusterRadius == j)) = size(fis.Rules, 2);

        % Calculate the mean of each metric after the 5-fold cross validation 
        metricsValues(iCount,jCount,1) = sum(cvNMSE)/folds;
        metricsValues(iCount,jCount,2) = sum(cvRMSE)/folds;
        metricsValues(iCount,jCount,3) = sum(cvR2)/folds;
        metricsValues(iCount,jCount,4) = sum(cvNDEI)/folds;

        jCount = jCount+1 ;
    end
    iCount = iCount+1 ;
end

%% Plot: MSE vs Number of Rules
figure;
grid on;
% Flatten matrices to row vectors for plotting
scatter(reshape(numRulesMat, 1, []), reshape(sqrt(MSEValues), 1, []), 'red', 'filled');
[minVal, minIndex] = min(sqrt(MSEValues), [], 'all');  % Find the minimum MSE value
[minRow, minCol] = ind2sub(size(MSEValues), minIndex);  % Get the row and column of the minimum MSE
yline(minVal, 'LineStyle', '--', 'Label', 'MSE Minimum value', 'LabelHorizontalAlignment', 'center');
xline(numRulesMat(minRow, minCol), 'LineStyle', '--', 'Label', ['MSE Minimum = ' num2str(numRulesMat(minRow, minCol))], 'LabelHorizontalAlignment', 'left');
xlabel('Number of Rules', 'Interpreter', 'latex');
ylabel('RMSE', 'Interpreter', 'latex');
title('Relationship between Number of Rules and Mean Error', 'Interpreter', 'latex');


%% 3D Plot: MSE as a function of number of features and cluster radius
figure;
[xAxisFeatures, yAxisRadius] = meshgrid(numFeatures, clusterRadius);  % Create a grid for plotting
surf(xAxisFeatures, yAxisRadius, MSEValues');  % Surface plot of MSE
xlabel('Number of Features', 'Interpreter', 'latex');
ylabel('Cluster Radius', 'Interpreter', 'latex');
zlabel('MSE', 'Interpreter', 'latex');
title('Relationship between Cluster Radius, Number of Features and Mean Squared Error', 'Interpreter', 'latex');

%% Plot the 3-D graph between mean errors, features number and R_values
% Create a grid of (R_Values, feature numbers) combinations
[R, Features] = meshgrid(clusterRadius, numFeatures);
% Reshape the error array to match the grid
errors = reshape(MSEValues, numel(clusterRadius), numel(numFeatures));

% Create a 3D scatter plot
figure;
scatter3(R(:), Features(:), sqrt(errors(:)), 'filled');
xlabel('Radius of clusters', 'Interpreter', 'latex');
ylabel('Number of Features', 'Interpreter', 'latex');
zlabel('Mean Error');
title('Relationship between Cluster Radius, Number of Features and Mean Error', 'Interpreter', 'latex');

%% Find the optimal TSK model
[minError, minErrorIndex] = min(errors(:));
[optimalNumFeaturesInd, optimalClusterRadiusInd] = ind2sub(size(errors), minErrorIndex);
optimalClusterRadius = clusterRadius(optimalClusterRadiusInd);
optimalNumFeatures = numFeatures(optimalNumFeaturesInd);

% Print the optimal parameters
fprintf('Optimal Cluster Radius: %.2f\n', optimalClusterRadius);
fprintf('--------------------------\n');
fprintf('Optimal Number of Features: %d\n', optimalNumFeatures);
fprintf('Minimum Error (MSE): %.4f\n', minError);